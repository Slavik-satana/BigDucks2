{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f8ae70-9643-4631-b8fd-b3d4393c6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abcca274-5578-4aa1-b618-12b786cb1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that can print the trainable parameters \n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19aac8ee-8702-41fe-8e7b-4ded28369dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 6508 samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>давай по россии значит на коленях быстро блять...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ну разве можно так с телефоном поступает</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>у меня нет с собой в полном адресе я щас дома ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>а я здесь кто я санитар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>дежурный по кузьминскому военнокомату</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>не понял</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>че и так</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>я записываю сигнал</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>я записала а вы там наговорили а номер кто</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>ааа так так так</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>это вы с меня названиваю двадцать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>вы откуда звоните вы из психушки</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>пытается расишку поставить наколения но зачем ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>нет ну у человека могут быть своим связью</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>ну а родители вообще довольны вообще ну</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>але але вы были у вас танки на сегодня</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>без мата пожалуйста вы не сказали ничего куда ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>город назовите куда вы звоните</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>встречаемся чтобы эти в еблотвое засунуло были...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>добрый у меня спросил вы мне телеграмму диктуете</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text\n",
       "0       1  давай по россии значит на коленях быстро блять...\n",
       "1       0           ну разве можно так с телефоном поступает\n",
       "2       0  у меня нет с собой в полном адресе я щас дома ...\n",
       "3       0                            а я здесь кто я санитар\n",
       "4       0              дежурный по кузьминскому военнокомату\n",
       "5       0                                           не понял\n",
       "6       1                                           че и так\n",
       "7       0                                 я записываю сигнал\n",
       "8       1         я записала а вы там наговорили а номер кто\n",
       "9       0                                    ааа так так так\n",
       "10      1                  это вы с меня названиваю двадцать\n",
       "11      0                   вы откуда звоните вы из психушки\n",
       "12      1  пытается расишку поставить наколения но зачем ...\n",
       "13      0          нет ну у человека могут быть своим связью\n",
       "14      0            ну а родители вообще довольны вообще ну\n",
       "15      1             але але вы были у вас танки на сегодня\n",
       "16      0  без мата пожалуйста вы не сказали ничего куда ...\n",
       "17      0                     город назовите куда вы звоните\n",
       "18      1  встречаемся чтобы эти в еблотвое засунуло были...\n",
       "19      0   добрый у меня спросил вы мне телеграмму диктуете"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.read_csv('hackaton_result_dataset.csv', sep=';')\n",
    "full_data['text'] = full_data['model_annotation']\n",
    "full_data = full_data.drop(['model_annotation', 'human_markup', 'audio_path', 'Unnamed: 4'], axis=1)\n",
    "print(f\"We have {len(full_data)} samples\") # Number of data we have\n",
    "full_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc18c42-d9a7-4bd6-b99e-aac5d1630f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 4555 training samples\n",
      "We have 1953 validation samples\n",
      "----------------------------\n",
      "Number of Essays written by Human: 3902\n",
      "Number of Essays generated by LLM: 2606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split it when augmented data is ready\n",
    "X_train, X_val, y_train, y_val = train_test_split(full_data[\"text\"],\n",
    "                                                  full_data[\"label\"],\n",
    "                                                  test_size=0.3,\n",
    "                                                  stratify=full_data[\"label\"],\n",
    "                                                  random_state=100500)\n",
    "print(f\"We have {len(X_train)} training samples\")\n",
    "print(f\"We have {len(X_val)} validation samples\")\n",
    "print(\"----------------------------\")\n",
    "count = full_data[\"label\"].value_counts()\n",
    "print(f\"Number of Essays written by Human: {count[0]}\")\n",
    "print(f\"Number of Essays generated by LLM: {count[1]}\")\n",
    "\n",
    "X_train.reset_index(drop = True, inplace = True)\n",
    "y_train.reset_index(drop = True, inplace = True)\n",
    "X_val.reset_index(drop = True, inplace = True)\n",
    "y_val.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fcfe9a6-323f-4e73-a8bf-7cd2e6c643e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'ai-forever/ruBert-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, return_dict=True, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a70d626-42c0-4989-94fb-140351038c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 443906\n",
      "all model parameters: 178752772\n",
      "percentage of trainable model parameters: 0.25%\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank Number\n",
    "    lora_alpha=32, # Alpha (Scaling Factor)\n",
    "    lora_dropout=0.05, # Dropout Prob for Lora\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # Which layer to apply LoRA, usually only apply on MultiHead Attention Layer\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_CLS # Seqence to Classification Task\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Reduced trainble parameters\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ee976f-008a-46d0-b2e0-0ce0f3767752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize_func(data):\n",
    "    return tokenizer(\n",
    "            data['texts'],\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4df419-fb42-4820-9ce4-010be3a925f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd5a53fa56a447e91af241d91526aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 4555\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.Dataset.from_pandas(pd.DataFrame({\"texts\":X_train,\"labels\":y_train}))\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    remove_columns=[\"texts\"]\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34767a2a-4ff9-4f3e-89fe-b71497327594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b14514910df48af8aeda195657adf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1953\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the Validation Data\n",
    "val_dataset = datasets.Dataset.from_pandas(pd.DataFrame({\"texts\":X_val,\"labels\":y_val}))\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    remove_columns=[\"texts\"]\n",
    ")\n",
    "\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe92c2e-94de-4f61-9ef3-6a2b03ec98df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps: 3600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='481' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [481/900 17:59 < 15:44, 0.44 it/s, Epoch 26.67/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val-auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>0.667827</td>\n",
       "      <td>0.502301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.659067</td>\n",
       "      <td>0.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>0.649475</td>\n",
       "      <td>0.529153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.653300</td>\n",
       "      <td>0.635560</td>\n",
       "      <td>0.555143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.633000</td>\n",
       "      <td>0.616163</td>\n",
       "      <td>0.586239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.614400</td>\n",
       "      <td>0.591092</td>\n",
       "      <td>0.639273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.588300</td>\n",
       "      <td>0.571697</td>\n",
       "      <td>0.666335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>0.562186</td>\n",
       "      <td>0.675932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.538600</td>\n",
       "      <td>0.552133</td>\n",
       "      <td>0.687428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.526900</td>\n",
       "      <td>0.548702</td>\n",
       "      <td>0.689990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.537546</td>\n",
       "      <td>0.711072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.558121</td>\n",
       "      <td>0.680425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.531010</td>\n",
       "      <td>0.720448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.551535</td>\n",
       "      <td>0.692565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.503800</td>\n",
       "      <td>0.554880</td>\n",
       "      <td>0.690653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.544553</td>\n",
       "      <td>0.696822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.530595</td>\n",
       "      <td>0.717265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.475600</td>\n",
       "      <td>0.549872</td>\n",
       "      <td>0.701098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>0.544293</td>\n",
       "      <td>0.709185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.537291</td>\n",
       "      <td>0.717061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.547470</td>\n",
       "      <td>0.708339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.535844</td>\n",
       "      <td>0.721743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.537780</td>\n",
       "      <td>0.719612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.453300</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.705145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.553226</td>\n",
       "      <td>0.708342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.544973</td>\n",
       "      <td>0.719194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.558149</td>\n",
       "      <td>0.712169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>0.589089</td>\n",
       "      <td>0.702171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.437800</td>\n",
       "      <td>0.559366</td>\n",
       "      <td>0.716632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.560249</td>\n",
       "      <td>0.714081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.558920</td>\n",
       "      <td>0.710893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.415400</td>\n",
       "      <td>0.562983</td>\n",
       "      <td>0.711536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.566431</td>\n",
       "      <td>0.710463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.581414</td>\n",
       "      <td>0.703222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.406600</td>\n",
       "      <td>0.588455</td>\n",
       "      <td>0.699392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.587371</td>\n",
       "      <td>0.702795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.377700</td>\n",
       "      <td>0.605010</td>\n",
       "      <td>0.699609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>0.605693</td>\n",
       "      <td>0.697054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.365300</td>\n",
       "      <td>0.580075</td>\n",
       "      <td>0.714079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.590757</td>\n",
       "      <td>0.707050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>0.624196</td>\n",
       "      <td>0.697691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.361500</td>\n",
       "      <td>0.608063</td>\n",
       "      <td>0.704070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.600998</td>\n",
       "      <td>0.706402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.621496</td>\n",
       "      <td>0.701935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.653921</td>\n",
       "      <td>0.695784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.635775</td>\n",
       "      <td>0.695136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.620775</td>\n",
       "      <td>0.703432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/8 00:01 < 00:03, 1.33 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Eval Metric\n",
    "def metrics(eval_prediction):\n",
    "    logits, labels = eval_prediction\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    auc_score = roc_auc_score(labels, pred)\n",
    "    return {\"Val-AUC\": auc_score}\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "# Define training Args\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir='./result-distilbert-lora',\n",
    "    logging_dir='./logs-distilbert-lora',\n",
    "#     auto_find_batch_size=True,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=train_batch_size, # You can adjust this value base on your available GPU, You may encounter \"out of memory\" error if this value is too lartge\n",
    "    per_device_eval_batch_size=eval_batch_size, # You can adjust this value base on your available GPU, You may encounter \"out of memory\" error if this value is too lartge\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    seed=100500,\n",
    "    fp16=True, # Only use with GPU\n",
    "    report_to='none'\n",
    ")   \n",
    "\n",
    "# Define Optimzer\n",
    "optimizer = AdamW(peft_model.parameters(), \n",
    "                  lr=1e-4,\n",
    "                  no_deprecation_warning=True)\n",
    "\n",
    "# Define Scheduler\n",
    "n_epochs = peft_training_args.num_train_epochs\n",
    "total_steps = n_epochs * math.ceil(len(train_dataset) / train_batch_size / 2)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps)\n",
    "\n",
    "# Data Collator\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, \n",
    "    padding=\"longest\"\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=train_dataset, # Training Data\n",
    "    eval_dataset=val_dataset, # Evaluation Data\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=metrics,\n",
    "    optimizers=(optimizer,lr_scheduler),\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "print(f\"Total Steps: {total_steps}\")\n",
    "# Train the model\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc3baab6-e6ff-4bde-8249-35453240596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('peft-lora-local-bs16-epoch5/tokenizer_config.json',\n",
       " 'peft-lora-local-bs16-epoch5/special_tokens_map.json',\n",
       " 'peft-lora-local-bs16-epoch5/vocab.txt',\n",
       " 'peft-lora-local-bs16-epoch5/added_tokens.json',\n",
       " 'peft-lora-local-bs16-epoch5/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to save the fine-tuned model\n",
    "peft_model_path=\"peft-lora-local-bs32-epoch50\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669e6f3-4b21-483c-8a88-2b821bf168f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fine_tuned_model(peft_model_path):\n",
    "    peft_model_base = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, return_dict=True, num_labels=2)\n",
    "    peft_tokenizer = AutoTokenizer.from_pretrained(peft_model_path)\n",
    "\n",
    "    peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                           peft_model_path,\n",
    "                                           is_trainable=False)\n",
    "\n",
    "    return peft_model, peft_tokenizer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
