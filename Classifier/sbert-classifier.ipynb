{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fine-tuned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hackaton_result_dataset.csv', encoding='windows-1251')\n",
    "val_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "val_df = val_df.head(5000)\n",
    "test_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = 'ai-forever/sbert_large_nlu_ru'\n",
    "TUNED_MODEL = 'models/sbert-v2/checkpoint-814'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "embed_model = AutoModel.from_pretrained(TUNED_MODEL)\n",
    "embed_model.to(device)\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 1024)\n",
      "(5000, 1024)\n",
      "(10000, 1024)\n"
     ]
    }
   ],
   "source": [
    "texts = df['model_annotation'].to_list()\n",
    "X_train = np.array([embed_bert_cls(text, embed_model, tokenizer) for text in texts])\n",
    "\n",
    "val_texts = val_df['annotation_fastconformer'].to_list()\n",
    "X_val = np.array([embed_bert_cls(text, embed_model, tokenizer) for text in val_texts])\n",
    "\n",
    "test_texts = test_df['annotation_fastconformer'].to_list()\n",
    "X_test = np.array([embed_bert_cls(text, embed_model, tokenizer) for text in test_texts])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/ft-embed/train-embeddings.npy', X_train)\n",
    "np.save('data/ft-embed/validation-embeddings.npy', X_val)\n",
    "np.save('data/ft-embed/test-embeddings.npy', X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load general embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hackaton_result_dataset.csv', encoding='windows-1251')\n",
    "val_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "val_df = val_df.head(5000)\n",
    "test_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = 'ai-forever/sbert_large_nlu_ru'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "embed_model = AutoModel.from_pretrained(BASE_MODEL)\n",
    "embed_model.to(device)\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 1024)\n",
      "(5000, 1024)\n",
      "(10000, 1024)\n"
     ]
    }
   ],
   "source": [
    "texts = df['model_annotation'].to_list()\n",
    "X_train = np.array([embed_bert_cls(text, embed_model, tokenizer) for text in texts])\n",
    "\n",
    "val_texts = val_df['annotation_fastconformer'].to_list()\n",
    "X_val = np.array([embed_bert_cls(text, embed_model, tokenizer) for text in val_texts])\n",
    "\n",
    "test_texts = test_df['annotation_fastconformer'].to_list()\n",
    "X_test = np.array([embed_bert_cls(text, embed_model, tokenizer) for text in test_texts])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/gen-embed/train-embeddings.npy', X_train)\n",
    "np.save('data/gen-embed/validation-embeddings.npy', X_val)\n",
    "np.save('data/gen-embed/test-embeddings.npy', X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general embeddings + catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hackaton_result_dataset.csv', encoding='windows-1251')\n",
    "val_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "val_df = val_df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 1024)\n",
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load('data/gen-embed/train-embeddings.npy')\n",
    "y_train = df['label'].to_list()\n",
    "\n",
    "X_val = np.load('data/gen-embed/validation-embeddings.npy')\n",
    "y_val = val_df['label'].to_list()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c389c3cbb3146c9aadca0ffd323b34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f35aa249b90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.05, loss_function='Logloss',\n",
    "                           custom_metric=['AUC'], depth=4, l2_leaf_reg=10, \n",
    "                           random_seed=SEED, task_type=\"GPU\", devices='0')\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_val, y_val),\n",
    "    verbose=False,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7075}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "test_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "\n",
    "X_test = np.load('data/gen-embed/test-embeddings.npy')\n",
    "y_test = test_df['label'].to_list()\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "metric.compute(references=y_test, predictions=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-tuned embeddings + catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hackaton_result_dataset.csv', encoding='windows-1251')\n",
    "val_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "val_df = val_df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 1024)\n",
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "texts = df['model_annotation'].to_list()\n",
    "X_train = np.load('data/ft-embed/train-embeddings.npy')\n",
    "y_train = df['label'].to_list()\n",
    "\n",
    "val_texts = val_df['annotation_fastconformer'].to_list()\n",
    "X_val = np.load('data/ft-embed/validation-embeddings.npy')\n",
    "y_val = val_df['label'].to_list()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053f93dbb2e74038aa953b0542e5f75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f35ab4cca90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.002, loss_function='Logloss',\n",
    "                           custom_metric=['AUC'], depth=4, l2_leaf_reg=20,\n",
    "                           random_seed=SEED, task_type=\"GPU\", devices='0')\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_val, y_val),\n",
    "    verbose=False,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7457}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "test_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "\n",
    "X_test = np.load('data/ft-embed/test-embeddings.npy')\n",
    "y_test = test_df['label'].to_list()\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "metric.compute(references=y_test, predictions=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hackaton_result_dataset.csv', encoding='windows-1251')\n",
    "val_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "val_df = val_df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 1024)\n",
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load('data/ft-embed/train-embeddings.npy')\n",
    "y_train = df['label'].to_list()\n",
    "\n",
    "X_val = np.load('data/ft-embed/validation-embeddings.npy')\n",
    "y_val = val_df['label'].to_list()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset (torch.utils.data.Dataset):\n",
    "    def __init__ (self, X, y):\n",
    "        self.X = torch.Tensor(X.reshape((-1, 32, 32))[:, np.newaxis, :, :])\n",
    "        self.y = torch.Tensor(y).to(torch.int8).tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__ (self, index):\n",
    "        return (self.X[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset(X_train, y_train)\n",
    "val_set = MyDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           shuffle=True, num_workers=num_workers)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 32 x 32 x 1\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, \n",
    "                                     stride=1, padding='same')\n",
    "        # 32 x 32 x 16\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=None, padding=0)\n",
    "        # 16 x 16 x 16\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3,\n",
    "                                     stride=1, padding='same')\n",
    "        # 16 x 16 x 32\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=None, padding=0)\n",
    "        # 8 x 8 x 32\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,\n",
    "                                     stride=1, padding='same')\n",
    "        # 8 x 8 x 64\n",
    "        self.pool3 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=None, padding=0)\n",
    "        # 4 x 4 x 64\n",
    "        self.fc = torch.nn.Linear(64 * 4 * 4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        x = self.pool2(self.conv2(x))\n",
    "        x = self.pool3(self.conv3(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        # x = F.relu(self.fc(x))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "class EfficientNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 32 x 32 x 1\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, \n",
    "                                     stride=1, padding='same')\n",
    "        # 32 x 32 x 3\n",
    "        self.efficientnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.DEFAULT)\n",
    "        self.fc = torch.nn.Linear(self.efficientnet.classifier[1].out_features, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.conv1(x)\n",
    "        x = self.efficientnet(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, train_dataloader, val_dataloader, criterion, optimizer, \n",
    "          scheduler=None, epochs=10, device='cpu', checkpoint_epochs=10):\n",
    "    start = time.time()\n",
    "    print(f'Training for {epochs} epochs on {device}')\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # put network in train mode for Dropout and Batch Normalization\n",
    "        net.train()\n",
    "        # loss and accuracy tensors are on the GPU to avoid data transfers\n",
    "        train_loss = torch.tensor(0., device=device)  \n",
    "        train_accuracy = torch.tensor(0., device=device)\n",
    "        for X, y in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = net(X)\n",
    "            loss = criterion(preds, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_loss += loss * train_dataloader.batch_size\n",
    "                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            # put network in train mode for Dropout and Batch Normalization\n",
    "            net.eval()\n",
    "            valid_loss = torch.tensor(0., device=device)\n",
    "            valid_accuracy = torch.tensor(0., device=device)\n",
    "            with torch.no_grad():\n",
    "                for X, y in val_dataloader:\n",
    "                    X = X.to(device)\n",
    "                    y = y.to(device)\n",
    "                    preds = net(X)\n",
    "                    loss = criterion(preds, y)\n",
    "\n",
    "                    valid_loss += loss * val_dataloader.batch_size\n",
    "                    valid_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n",
    "        \n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "            \n",
    "        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n",
    "        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            print(f'Valid loss: {valid_loss/len(val_dataloader.dataset):.2f}')\n",
    "            print(f'Valid accuracy: {100*valid_accuracy/len(val_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if epoch%checkpoint_epochs==0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, './checkpoint.pth.tar')\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Total training time: {end-start:.1f} seconds')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 25 epochs on cuda\n",
      "Epoch 1/25\n",
      "Training loss: 0.78\n",
      "Training accuracy: 53.90\n",
      "Valid loss: 5.25\n",
      "Valid accuracy: 56.16\n",
      "\n",
      "Epoch 2/25\n",
      "Training loss: 0.71\n",
      "Training accuracy: 58.24\n",
      "Valid loss: 4.20\n",
      "Valid accuracy: 59.36\n",
      "\n",
      "Epoch 3/25\n",
      "Training loss: 0.68\n",
      "Training accuracy: 60.03\n",
      "Valid loss: 189.95\n",
      "Valid accuracy: 59.86\n",
      "\n",
      "Epoch 4/25\n",
      "Training loss: 0.65\n",
      "Training accuracy: 63.32\n",
      "Valid loss: 0.95\n",
      "Valid accuracy: 61.52\n",
      "\n",
      "Epoch 5/25\n",
      "Training loss: 0.63\n",
      "Training accuracy: 65.75\n",
      "Valid loss: 4.86\n",
      "Valid accuracy: 63.08\n",
      "\n",
      "Epoch 6/25\n",
      "Training loss: 0.60\n",
      "Training accuracy: 68.67\n",
      "Valid loss: 9.73\n",
      "Valid accuracy: 63.16\n",
      "\n",
      "Epoch 7/25\n",
      "Training loss: 0.59\n",
      "Training accuracy: 69.11\n",
      "Valid loss: 2.15\n",
      "Valid accuracy: 64.70\n",
      "\n",
      "Epoch 8/25\n",
      "Training loss: 0.57\n",
      "Training accuracy: 71.68\n",
      "Valid loss: 4.40\n",
      "Valid accuracy: 64.74\n",
      "\n",
      "Epoch 9/25\n",
      "Training loss: 0.56\n",
      "Training accuracy: 72.03\n",
      "Valid loss: 1.48\n",
      "Valid accuracy: 66.34\n",
      "\n",
      "Epoch 10/25\n",
      "Training loss: 0.54\n",
      "Training accuracy: 73.52\n",
      "Valid loss: 6.36\n",
      "Valid accuracy: 66.80\n",
      "\n",
      "Epoch 11/25\n",
      "Training loss: 0.53\n",
      "Training accuracy: 74.48\n",
      "Valid loss: 13.43\n",
      "Valid accuracy: 66.56\n",
      "\n",
      "Epoch 12/25\n",
      "Training loss: 0.52\n",
      "Training accuracy: 75.09\n",
      "Valid loss: 1.48\n",
      "Valid accuracy: 67.44\n",
      "\n",
      "Epoch 13/25\n",
      "Training loss: 0.50\n",
      "Training accuracy: 76.63\n",
      "Valid loss: 7.04\n",
      "Valid accuracy: 67.40\n",
      "\n",
      "Epoch 14/25\n",
      "Training loss: 0.49\n",
      "Training accuracy: 77.11\n",
      "Valid loss: 5.35\n",
      "Valid accuracy: 68.00\n",
      "\n",
      "Epoch 15/25\n",
      "Training loss: 0.48\n",
      "Training accuracy: 77.74\n",
      "Valid loss: 1.61\n",
      "Valid accuracy: 68.72\n",
      "\n",
      "Epoch 16/25\n",
      "Training loss: 0.47\n",
      "Training accuracy: 79.15\n",
      "Valid loss: 2.56\n",
      "Valid accuracy: 68.48\n",
      "\n",
      "Epoch 17/25\n",
      "Training loss: 0.45\n",
      "Training accuracy: 79.33\n",
      "Valid loss: 5.56\n",
      "Valid accuracy: 68.84\n",
      "\n",
      "Epoch 18/25\n",
      "Training loss: 0.44\n",
      "Training accuracy: 80.55\n",
      "Valid loss: 6.30\n",
      "Valid accuracy: 69.42\n",
      "\n",
      "Epoch 19/25\n",
      "Training loss: 0.44\n",
      "Training accuracy: 79.66\n",
      "Valid loss: 1.75\n",
      "Valid accuracy: 69.34\n",
      "\n",
      "Epoch 20/25\n",
      "Training loss: 0.43\n",
      "Training accuracy: 81.10\n",
      "Valid loss: 3.12\n",
      "Valid accuracy: 69.74\n",
      "\n",
      "Epoch 21/25\n",
      "Training loss: 0.42\n",
      "Training accuracy: 81.39\n",
      "Valid loss: 3.38\n",
      "Valid accuracy: 70.20\n",
      "\n",
      "Epoch 22/25\n",
      "Training loss: 0.41\n",
      "Training accuracy: 81.93\n",
      "Valid loss: 3.66\n",
      "Valid accuracy: 70.36\n",
      "\n",
      "Epoch 23/25\n",
      "Training loss: 0.41\n",
      "Training accuracy: 82.74\n",
      "Valid loss: 1.61\n",
      "Valid accuracy: 70.12\n",
      "\n",
      "Epoch 24/25\n",
      "Training loss: 0.39\n",
      "Training accuracy: 82.36\n",
      "Valid loss: 5.90\n",
      "Valid accuracy: 70.28\n",
      "\n",
      "Epoch 25/25\n",
      "Training loss: 0.39\n",
      "Training accuracy: 82.76\n",
      "Valid loss: 6.28\n",
      "Valid accuracy: 70.86\n",
      "\n",
      "Total training time: 663.6 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (efficientnet): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.005, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.01, mode=row)\n",
       "        )\n",
       "        (1): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)\n",
       "        )\n",
       "        (2): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
       "        )\n",
       "        (3): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)\n",
       "        )\n",
       "        (1): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.035, mode=row)\n",
       "        )\n",
       "        (2): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
       "        )\n",
       "        (3): FusedMBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.045, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.065, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07, mode=row)\n",
       "        )\n",
       "        (5): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.075, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "              (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.085, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.095, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (5): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)\n",
       "        )\n",
       "        (6): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)\n",
       "        )\n",
       "        (7): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)\n",
       "        )\n",
       "        (8): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.135, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14500000000000002, mode=row)\n",
       "        )\n",
       "        (5): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15, mode=row)\n",
       "        )\n",
       "        (6): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.155, mode=row)\n",
       "        )\n",
       "        (7): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
       "        )\n",
       "        (8): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.165, mode=row)\n",
       "        )\n",
       "        (9): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17, mode=row)\n",
       "        )\n",
       "        (10): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.175, mode=row)\n",
       "        )\n",
       "        (11): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n",
       "        )\n",
       "        (12): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.185, mode=row)\n",
       "        )\n",
       "        (13): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19, mode=row)\n",
       "        )\n",
       "        (14): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.195, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=True)\n",
       "      (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=1000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net = ConvNet().to(device)\n",
    "net = EfficientNet().to(device)\n",
    "\n",
    "efficientnet_params = [param for name, param in net.named_parameters() \n",
    "                       if 'fc' not in str(name) and 'conv1' not in str(name)]\n",
    "added_params = [param for name, param in net.named_parameters() \n",
    "                if 'fc' in str(name) or 'conv1' in str(name)]\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([{'params':efficientnet_params},\n",
    "                              {'params': added_params, 'lr': 1e-6}],\n",
    "                             lr=1e-5)\n",
    "\n",
    "train(net, train_loader, val_loader, criterion, optimizer, epochs=25, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "\n",
    "X_test = np.load('data/ft-embed/test-embeddings.npy')\n",
    "y_test = test_df['label'].to_list()\n",
    "test_set = MyDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set.X, batch_size=16,\n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_dataloader, device='cpu'):\n",
    "    test_labels = []\n",
    "    proba = []\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_dataloader):\n",
    "            outputs = net(data.to(device))\n",
    "            _, preds = torch.max(outputs.data , 1)\n",
    "            test_labels.append(preds.cpu().tolist())\n",
    "            proba.append(outputs.cpu().tolist())\n",
    "    test_labels = sum(test_labels, [])\n",
    "    proba = sum(proba, [])\n",
    "    return proba, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:09<00:00, 64.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7051}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "proba, preds = predict(net, test_loader, device=device)\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "metric.compute(references=y_test, predictions=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hackaton_result_dataset.csv', encoding='windows-1251')\n",
    "val_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "val_df = val_df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 1024)\n",
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load('data/ft-embed/train-embeddings.npy')\n",
    "y_train = df['label'].to_list()\n",
    "\n",
    "X_val = np.load('data/ft-embed/validation-embeddings.npy')\n",
    "y_val = val_df['label'].to_list()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset (torch.utils.data.Dataset):\n",
    "    def __init__ (self, X, y):\n",
    "        self.X = torch.Tensor(X)\n",
    "        # if BCE\n",
    "        self.y = torch.Tensor(y).unsqueeze(1)\n",
    "        # if crossentropy\n",
    "        # self.y = torch.Tensor(y).to(torch.int8).tolist()\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__ (self, index):\n",
    "        return (self.X[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset(X_train, y_train)\n",
    "val_set = MyDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           shuffle=True, num_workers=num_workers)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.layer1 = torch.nn.Linear(1024, 512)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.dropout2 = torch.nn.Dropout(0.1)\n",
    "        self.layer2 = torch.nn.Linear(512, 256)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        self.dropout3 = torch.nn.Dropout(0.1)\n",
    "        self.layer3 = torch.nn.Linear(256, 128)\n",
    "        self.act3 = torch.nn.ReLU()\n",
    "        self.dropout4 = torch.nn.Dropout(0.1)\n",
    "        self.output = torch.nn.Linear(128, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.dropout4(x)\n",
    "        # x = self.output(x)\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_dataloader, val_dataloader, criterion, optimizer, \n",
    "          scheduler=None, epochs=10, device='cpu', checkpoint_epochs=10):\n",
    "    start = time.time()\n",
    "    print(f'Training for {epochs} epochs on {device}')\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # put network in train mode for Dropout and Batch Normalization\n",
    "        net.train()\n",
    "        # loss and accuracy tensors are on the GPU to avoid data transfers\n",
    "        train_loss = torch.tensor(0., device=device)  \n",
    "        train_accuracy = torch.tensor(0., device=device)\n",
    "        for X, y in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = net(X)\n",
    "            loss = criterion(preds, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_loss += loss * train_dataloader.batch_size\n",
    "                # train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n",
    "                train_accuracy += (torch.where(preds>0.5, 1, 0) == y).sum()\n",
    "                # y_pred = preds.cpu().numpy()\n",
    "                # y_true = y.cpu().numpy()\n",
    "                # train_accuracy += (np.where(y_pred>0.5, 1, 0) == y_true).sum()\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            # put network in train mode for Dropout and Batch Normalization\n",
    "            net.eval()\n",
    "            valid_loss = torch.tensor(0., device=device)\n",
    "            valid_accuracy = torch.tensor(0., device=device)\n",
    "            with torch.no_grad():\n",
    "                for X, y in val_dataloader:\n",
    "                    X = X.to(device)\n",
    "                    y = y.to(device)\n",
    "                    preds = net(X)\n",
    "                    loss = criterion(preds, y)\n",
    "                    valid_loss += loss * val_dataloader.batch_size\n",
    "                    # valid_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n",
    "                    valid_accuracy += (torch.where(preds>0.5, 1, 0) == y).sum()\n",
    "                    # y_pred = preds.cpu().numpy()\n",
    "                    # y_true = y.cpu().numpy()\n",
    "                    # valid_accuracy += (np.where(y_pred>0.5, 1, 0) == y_true).sum()\n",
    "                    \n",
    "        \n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "            \n",
    "        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n",
    "        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            print(f'Valid loss: {valid_loss/len(val_dataloader.dataset):.2f}')\n",
    "            print(f'Valid accuracy: {100*valid_accuracy/len(val_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if epoch%checkpoint_epochs==0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, './checkpoint.pth.tar')\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Total training time: {end-start:.1f} seconds')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs on cuda\n",
      "Epoch 1/20\n",
      "Training loss: 0.67\n",
      "Training accuracy: 72.03\n",
      "Valid loss: 0.65\n",
      "Valid accuracy: 74.40\n",
      "\n",
      "Epoch 2/20\n",
      "Training loss: 0.48\n",
      "Training accuracy: 88.94\n",
      "Valid loss: 0.53\n",
      "Valid accuracy: 74.80\n",
      "\n",
      "Epoch 3/20\n",
      "Training loss: 0.29\n",
      "Training accuracy: 89.51\n",
      "Valid loss: 0.57\n",
      "Valid accuracy: 74.60\n",
      "\n",
      "Epoch 4/20\n",
      "Training loss: 0.27\n",
      "Training accuracy: 89.81\n",
      "Valid loss: 0.60\n",
      "Valid accuracy: 74.50\n",
      "\n",
      "Epoch 5/20\n",
      "Training loss: 0.26\n",
      "Training accuracy: 89.80\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 74.62\n",
      "\n",
      "Epoch 6/20\n",
      "Training loss: 0.26\n",
      "Training accuracy: 89.81\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 74.76\n",
      "\n",
      "Epoch 7/20\n",
      "Training loss: 0.26\n",
      "Training accuracy: 89.86\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 74.66\n",
      "\n",
      "Epoch 8/20\n",
      "Training loss: 0.26\n",
      "Training accuracy: 89.94\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 74.82\n",
      "\n",
      "Epoch 9/20\n",
      "Training loss: 0.25\n",
      "Training accuracy: 90.15\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 74.98\n",
      "\n",
      "Epoch 10/20\n",
      "Training loss: 0.25\n",
      "Training accuracy: 90.01\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.06\n",
      "\n",
      "Epoch 11/20\n",
      "Training loss: 0.25\n",
      "Training accuracy: 90.24\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.16\n",
      "\n",
      "Epoch 12/20\n",
      "Training loss: 0.25\n",
      "Training accuracy: 90.32\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.10\n",
      "\n",
      "Epoch 13/20\n",
      "Training loss: 0.25\n",
      "Training accuracy: 90.38\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.20\n",
      "\n",
      "Epoch 14/20\n",
      "Training loss: 0.25\n",
      "Training accuracy: 90.52\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.22\n",
      "\n",
      "Epoch 15/20\n",
      "Training loss: 0.25\n",
      "Training accuracy: 90.20\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.06\n",
      "\n",
      "Epoch 16/20\n",
      "Training loss: 0.24\n",
      "Training accuracy: 90.61\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.26\n",
      "\n",
      "Epoch 17/20\n",
      "Training loss: 0.24\n",
      "Training accuracy: 90.61\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.22\n",
      "\n",
      "Epoch 18/20\n",
      "Training loss: 0.24\n",
      "Training accuracy: 90.64\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.08\n",
      "\n",
      "Epoch 19/20\n",
      "Training loss: 0.24\n",
      "Training accuracy: 90.67\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.36\n",
      "\n",
      "Epoch 20/20\n",
      "Training loss: 0.24\n",
      "Training accuracy: 90.83\n",
      "Valid loss: 0.61\n",
      "Valid accuracy: 75.14\n",
      "\n",
      "Total training time: 31.1 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (layer1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (act1): ReLU()\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (layer2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  (layer3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (act3): ReLU()\n",
       "  (dropout4): Dropout(p=0.1, inplace=False)\n",
       "  (output): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Perceptron().to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=1e-5)\n",
    "\n",
    "train(net, train_loader, val_loader, criterion, optimizer, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "\n",
    "X_test = np.load('data/ft-embed/test-embeddings.npy')\n",
    "y_test = test_df['label'].to_list()\n",
    "test_set = MyDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set.X, batch_size=16,\n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_dataloader, device='cpu'):\n",
    "    test_labels = []\n",
    "    proba = []\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_dataloader):\n",
    "            outputs = net(data.to(device))\n",
    "            preds = torch.flatten((outputs > 0.5).to(torch.int8))\n",
    "            test_labels.append(preds.cpu().tolist())\n",
    "            proba.append(torch.flatten(outputs).cpu().tolist())\n",
    "    test_labels = sum(test_labels, [])\n",
    "    proba = sum(proba, [])\n",
    "    return proba, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for crossentropy\n",
    "# def predict(net, test_dataloader, device='cpu'):\n",
    "#     test_labels = []\n",
    "#     proba = []\n",
    "#     net.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for data in tqdm(test_dataloader):\n",
    "#             outputs = net(data.to(device))\n",
    "#             _, preds = torch.max(outputs.data , 1)\n",
    "#             test_labels.append(preds.cpu().tolist())\n",
    "#             proba.append(outputs.cpu().tolist())\n",
    "#     test_labels = sum(test_labels, [])\n",
    "#     proba = sum(proba, [])\n",
    "#     return proba, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:00<00:00, 912.19it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.751}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "proba, preds = predict(net, test_loader, device=device)\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "metric.compute(references=y_test, predictions=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'models/percetron-classifier.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load('roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (layer1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (act1): ReLU()\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (layer2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  (layer3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (act3): ReLU()\n",
       "  (dropout4): Dropout(p=0.1, inplace=False)\n",
       "  (output): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Perceptron().to(device)\n",
    "model.load_state_dict(torch.load('models/percetron-classifier.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4018], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(test_set.X[0].to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/validation-dataset.csv', encoding='windows-1251')\n",
    "\n",
    "X_test = np.load('data/ft-embed/test-embeddings.npy')\n",
    "y_test = test_df['label'].to_list()\n",
    "test_set = MyDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set.X, batch_size=16,\n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_dataloader, device='cpu'):\n",
    "    test_labels = []\n",
    "    proba = []\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_dataloader):\n",
    "            outputs = net(data.to(device))\n",
    "            preds = torch.flatten((outputs > 0.5).to(torch.int8))\n",
    "            test_labels.append(preds.cpu().tolist())\n",
    "            proba.append(torch.flatten(outputs).cpu().tolist())\n",
    "    test_labels = sum(test_labels, [])\n",
    "    proba = sum(proba, [])\n",
    "    return proba, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:00<00:00, 907.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.8110031129934587}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba, preds = predict(model, test_loader, device=device)\n",
    "\n",
    "metric.compute(references=y_test, prediction_scores=proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-env311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
