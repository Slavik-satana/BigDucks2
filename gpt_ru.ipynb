{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T18:40:03.102886Z",
     "start_time": "2024-05-22T18:40:01.131155Z"
    }
   },
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:40:03.107369Z",
     "start_time": "2024-05-22T18:40:03.103392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ],
   "id": "16a7ab76199462f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:40:04.725893Z",
     "start_time": "2024-05-22T18:40:03.107369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name_or_path = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
   ],
   "id": "e92ba8eead100f7e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:40:05.083509Z",
     "start_time": "2024-05-22T18:40:04.725893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "train_dataset = TextDataset(tokenizer=tokenizer,file_path=\"hrd2.txt\",block_size=128)\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞–ª–æ–¥–µ—Ä–∞ (–Ω–∞—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–æ –¥–ª–∏–Ω–µ –∫—É—Å–∫–∏)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ],
   "id": "c2678314b89ed6d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grishin\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:40:05.546492Z",
     "start_time": "2024-05-22T18:40:05.084512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏\n",
    "training_args = TrainingArguments(\n",
    "    report_to=None,\n",
    "    output_dir=\"finetuned-gpt_ru\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size= 16,\n",
    "    gradient_accumulation_steps= 4, # –∫–æ–ø–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ batch\n",
    "    max_steps= 1000,\n",
    "    save_steps= 100,\n",
    "    eval_steps= 100,\n",
    "    dataloader_num_workers= 0,\n",
    "    save_total_limit= 2\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)\n",
    ")"
   ],
   "id": "37647f5b4ba7994f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:52:49.295953Z",
     "start_time": "2024-05-22T18:40:05.546492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.train()"
   ],
   "id": "8d269949d665bc9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 12:42, Epoch 111/112]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.068800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=3.4080736083984373, metrics={'train_runtime': 763.6564, 'train_samples_per_second': 83.807, 'train_steps_per_second': 1.309, 'total_flos': 4144418242560000.0, 'train_loss': 3.4080736083984373, 'epoch': 111.11111111111111})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:52:49.555396Z",
     "start_time": "2024-05-22T18:52:49.295953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
    "model_2 = GPT2LMHeadModel.from_pretrained('finetuned-gpt_ru/checkpoint-1000').to(DEVICE)"
   ],
   "id": "92edb2b68c958908",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:52:50.629739Z",
     "start_time": "2024-05-22T18:52:49.555396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "ques = '</s>'\n",
    "inputs = tokenizer(ques, return_tensors='pt').to(DEVICE)\n",
    "generated_token_ids = model_2.generate(\n",
    "    **inputs,\n",
    "    temperature=1.2,\n",
    "    max_new_tokens=128,\n",
    "    top_p=0.95,\n",
    "    top_k=10,\n",
    "    repetition_penalty=1.03,\n",
    "    length_penalty=1.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=50257,\n",
    ")"
   ],
   "id": "7f58de767dbfabce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grishin\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Grishin\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Grishin\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:52:50.632997Z",
     "start_time": "2024-05-22T18:52:50.629739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–≤–æ–¥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "out = tokenizer.decode(generated_token_ids[0][:])\n",
    "out.split('\\n')"
   ],
   "id": "368054eb2384b4c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " '—è –Ω–µ –∑–Ω–∞—é —á—Ç–æ –≤—ã —Ç–∞–º –¥–µ–ª–∞–ª–∏',\n",
       " '–Ω—É—É—É —è –Ω–µ –ø–æ–Ω–∏–º–∞—é –æ —á—ë–º –≤—ã –≥–æ–≤–æ—Ä–∏—Ç–µ',\n",
       " '–∞ —á—ë —Ç—ã —Ö–æ—á–µ—à—å –æ—Ç –º–µ–Ω—è',\n",
       " '–¥–∞–≤–∞–π –≤—Å—Ç—Ä–µ—Ç–∏–º—Å—è –∏ –ø–æ–≥–æ–≤–æ—Ä–∏–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ',\n",
       " '—Ç—ã —á—ë –ø—Ä–∏–¥—É—Ä–∏–≤–∞–µ—à—å—Å—è —á—Ç–æ –ª–∏',\n",
       " '–≤—ã –∫—É–¥–∞ –∑–≤–æ–Ω–∏—Ç–µ —Ç–æ',\n",
       " '–≤—Å—ë –¥–∞–≤–∞–π –¥–∞–≤–∞–π –≥–æ–≤–æ—Ä–∏ –∞–¥—Ä–µ—Å –¥–∞–≤–∞–π',\n",
       " '—Ç–∞–∫–æ–π –≤–æ—Ç –∫–∞–∑—É—Å',\n",
       " '–∏ —á—Ç–æ –¥–∞–ª—å—à–µ',\n",
       " '–≤–æ—Ç —Ç–∞–∫ –≤–æ—Ç',\n",
       " '–Ω–µ –ø–æ–Ω—è–ª',\n",
       " '—ç—Ç–æ –∫—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç',\n",
       " '–∫–∞–∫–æ–≥–æ –≤–æ–µ–Ω–∫–æ–º–∞—Ç–∞',\n",
       " '—É –≤–∞—Å –µ—Å—Ç—å –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞',\n",
       " '–Ω–µ—Ç —è —Å—Ç–æ—Ä–æ–∂ –≤–æ–µ–Ω–∫–æ–º–∞—Ç–∞',\n",
       " '—á—Ç–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ',\n",
       " '–ø–æ–Ω–∏–º–∞–µ—à—å —ç—Ç–æ –Ω–µ —à—É—Ç–∫–∏ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å',\n",
       " '—Å–ª—É—à–∞–π –Ω—É—É—É —Ç—ã –∂–µ –∑–Ω–∞–µ—à—å —á—Ç–æ —ç—ç—ç',\n",
       " '–Ω–∞—á–∞–ª—å–Ω–∏–∫—É –≤—Ç–æ—Ä–æ–≥–æ –æ—Ç–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–±—ã—Ç—å –≤ —Ç—Ä–∏—Å—Ç–∞ —á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—ã–π –∫–∞–±–∏–Ω–µ—Ç']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
